Postgres for Humans
===================

Source: https://www.youtube.com/watch?time_continue=80&v=MpH8W5hce9I

Learn:

* Datatypes
* Conditional Indexes
* Transactional DDL
* Foreign Data Wrappers
* Concurrent Index Creation
* Extensions
* Common Table Expression
* Fast Column Addition
* Listen / Notify
* Table Inheritance
* Per Transaction sync replication
* Window Function
* NoSQL inside SQL
* Momentum

OLTP vs OLAP
------------
**OLTP** equals mostly webapps. **OLAP** mostly equals business intelligence
and reporting.


Setup / Config
--------------
-> Talk Postgresql when it is not your day job.


Cache all the things
--------------------
PostgreSQL is very good in caching your data.
If you do not have a cache hit rate of 99%
you should add more memory to the box.
Try to get a cache hit rate of 99%:

.. code-block:: sql

    SELECT 
      sum(heap_blks_read) as heap_read,
      sum(heap_blks_hit)  as heap_hit,
      sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) as ratio
    FROM 
      pg_statio_user_tables;
        

Index Hit Rate
--------------
Should be greater than 95%.
Asks how often you use indexes to query the database:

.. code-block:: sql

    SELECT 
      relname, 
      100 * idx_scan / (seq_scan + idx_scan) percent_of_times_index_used, 
      n_live_tup rows_in_table
    FROM 
      pg_stat_user_tables
    WHERE 
        seq_scan + idx_scan > 0 
    ORDER BY 
      n_live_tup DESC;
      
You can try as well this query:

.. code-block:: sql

    SELECT
        relname,
        100 * idx_scan / (seq_scan + idx_scan),
        n_live_tup
    FROM pg_stat_user_tables
    ORDER BY n_live_tup DESC;
    
    
Use a `.psqlrc` file
--------------------
For example to define a function::

    \set show_slow_queries
    'SELECT
    (total_time / 1000 / 60) as total_minutes,
    (total_time/calls) as average_time, query
    FROM pg_stat_statements
    ORDER BY 1 DESC
    LIMIT 100;'    
    

Understand specific query performance
-------------------------------------
You always can put an `EXPLAIN` in front of a SQL
query and `EXPLAIN_ANALYZE` to get the real 
time of a query. You are mostly interested in the
**max** time in the analysis::

    actual time=startuptime..maxtime rows=1000
      
Common response times:

* Page Response time <100ms
* Common queries <10ms ==> aim at around 1ms
* Rare queries <100ms

Example for index creation:

.. code-block:: sql

    CREATE INDEX idx_emps ON employees (salary);
    

pg_stat_statements
------------------

Installation::

    # Replace 9.X with your installed Postgres version:
	sudo apt-get install postgresql-contrib-9.X

Add to your `postgres.conf`::

	shared_preload_libraries = 'pg_stat_statements'

	# Increase the max size of the query strings Postgres records
	track_activity_query_size = 2048

	# Track statements generated by stored procedures as well
	pg_stat_statements.track = all

Finally, restart the server::

    sudo service postgresql restart

PostgreSQL extension - normalizes every single query against your database
for proper inspection:

.. code-block:: sql

    select * from pg_stat_statements where query ~ 'from users where email = ?';

You can get stats about all queries:

.. code-block:: sql

	SELECT
		(total_time / 1000 / 60) as total,
		(total_time/calls) as avg,
		query
	FROM pg_stat_statements
	ORDER BY 1 DESC
	LIMIT 100;

Indexes
-------
There are a couple of index types and often it is hard
to decide which one should be used like:

* **B-Tree**: What you usually want.
* **Generalized Inverted Index (GIN)**: PostGIS, JSON, Array, HStore, something with multiple values in a column.
* **Generalized Search Tree (GIST)**: Fulltext search, Shapes (GIS),
* **K Nearest Neighbors (KNN)**: Similarity
* **Space Partitioned GIST (SP-GIST)**: Phone numbers,
* **VODKA**: JSON, JSONB, HStore,

Other indexes possibilities are:

* **Conditional**: For instance, if you want only big cities or the up-to-date place where somebody lives. If the application frequently requests only for big cities then you might use a conditional index.

.. code-block:: sql

    create index idx_large_population 
    on places(name) 
    where population > 10000;

               
* **Functional**: You can add an index on a PostgreSQL function:

.. code-block:: sql

    create index idx_large_population on
    places(get_numeric('pop', data));


.. important:: Concurrent creation
   Useful as you do **not** lock the whole table while creation.
   Generally, it is a bit slower but the database can be still used.
   So, use:

   `create index concurrent`


HStore & JSON
-------------
**HStore** is a key value store inside your database.
However, it is not a full document store like MongoDB.
Preferable indexes:

* GIN or
* GIST

Enabling **Hstore**:

.. code-block:: sql

    CREATE EXTENSION hstore;
	CREATE TABLE users (
	id integer NOT NULL,
	email character varying(255),
	data hstore,
	created_at timestamp without time zone,
	last_login timestamp without time zone
	);

**JSON** is still not fully useful in PostgreSQL. Mostly a comparison of text.
For better performance use `functional indexes`.

**JSONB** a binary representation of *JSON* on disk. Works generally much better
than normal *JSON* data type.


Connection Pooling
------------------
PostgreSQL is fine with about 100 to 200 concurrent connections. For more you are going
to need some connection pooling. Common options for connection pooling are:

* Application / framework layer
* Stand alone daemon

However, in the end you are fine if you use **pgbouncer** or **pgpool**.


Adding cache / replication
--------------------------
The easiest way is to scale up -> memory!
At a certain point you need to scale out with replicas.
Some data, but with a different cache.

Replication options (2014?!). How is the situation currently?:
* slony
* londiste
* bucardo
* pgpool
* wal-e (Heroku!)
* barman (!)


Backups
-------
There are *logical* and *physical* backups. 
Information is from 2014:

* **Logical** backups are human readable / portable and can be created with `pg_dump`.
  Good across different architectures, but has a load on the DB and works only up
  to 50 GB of data.

* **Physical** backups are bytes on a disk and it is not cross platform. It requires
  more initial setup and with limited portability. The load on the system is not as 
  high as with the *logical* backup. It should be used above 50GB. **Physical backups
  scale pretty well!** :)

Ways to solve (2017):

* **Barman**: http://www.pgbarman.org/
* **Continuous Archiving**: https://www.postgresql.org/docs/current/static/continuous-archiving.html
    
